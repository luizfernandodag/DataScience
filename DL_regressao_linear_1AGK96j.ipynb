{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_regressao_linear_1AGK96j.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luizfernandodag/DataScience/blob/master/DL_regressao_linear_1AGK96j.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47NbPBLo_OU7"
      },
      "source": [
        "#Regressão Linear"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEhH_vMxvxRU"
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "boston = load_boston()\n",
        "X, y = scale(boston.data), boston.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnkBVR1Kv82X",
        "outputId": "da4ad053-43ae-4a0a-b91e-54baa57976e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.linear_model import LinearRegression \n",
        "regression = LinearRegression() \n",
        "regression.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5FNKD8rwEIA",
        "outputId": "8fd6eaf5-3f6f-49bd-8639-018b89bbad6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"\n",
        "Agora que o algoritmo está instalado, você pode usar o método de \n",
        "pontuação para relatar a medida R2:\n",
        "\"\"\"\n",
        "print(f'R2 {regression.score(X, y):.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "R2 0.741\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nEpshmFzcYC",
        "outputId": "7667dbfb-5042-4ab1-fdcb-00cfcc37f672",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "elementos = [a + ':' + str(round(b, 1)) for a, b in zip(boston.feature_names, regression.coef_)]\n",
        "\n",
        "for elemento in elementos:\n",
        "  print(elemento)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CRIM:-0.9\n",
            "ZN:1.1\n",
            "INDUS:0.1\n",
            "CHAS:0.7\n",
            "NOX:-2.1\n",
            "RM:2.7\n",
            "AGE:0.0\n",
            "DIS:-3.1\n",
            "RAD:2.7\n",
            "TAX:-2.1\n",
            "PTRATIO:-2.1\n",
            "B:0.8\n",
            "LSTAT:-3.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMnk6g8MwqOk",
        "outputId": "d71c90c5-cec0-4dd1-9abb-fefd6582eb48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        }
      },
      "source": [
        "print(boston.DESCR)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".. _boston_dataset:\n",
            "\n",
            "Boston house prices dataset\n",
            "---------------------------\n",
            "\n",
            "**Data Set Characteristics:**  \n",
            "\n",
            "    :Number of Instances: 506 \n",
            "\n",
            "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
            "\n",
            "    :Attribute Information (in order):\n",
            "        - CRIM     per capita crime rate by town\n",
            "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
            "        - INDUS    proportion of non-retail business acres per town\n",
            "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
            "        - NOX      nitric oxides concentration (parts per 10 million)\n",
            "        - RM       average number of rooms per dwelling\n",
            "        - AGE      proportion of owner-occupied units built prior to 1940\n",
            "        - DIS      weighted distances to five Boston employment centres\n",
            "        - RAD      index of accessibility to radial highways\n",
            "        - TAX      full-value property-tax rate per $10,000\n",
            "        - PTRATIO  pupil-teacher ratio by town\n",
            "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
            "        - LSTAT    % lower status of the population\n",
            "        - MEDV     Median value of owner-occupied homes in $1000's\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "\n",
            "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
            "\n",
            "This is a copy of UCI ML housing dataset.\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
            "\n",
            "\n",
            "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
            "\n",
            "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
            "prices and the demand for clean air', J. Environ. Economics & Management,\n",
            "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
            "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
            "pages 244-261 of the latter.\n",
            "\n",
            "The Boston house-price data has been used in many machine learning papers that address regression\n",
            "problems.   \n",
            "     \n",
            ".. topic:: References\n",
            "\n",
            "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
            "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFj4GJko2Dcf",
        "outputId": "37bfda8d-d840-4283-ba3b-5ec2b40d30a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "# ONE HOT ENCODER VS LABEL ENCODER\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "lbl = LabelEncoder()\n",
        "enc = OneHotEncoder()\n",
        "qualitativo = ['vermelho', 'vermelho', 'verde', 'azul',\n",
        " 'vermelho', 'azul', 'azul', 'verde']\n",
        "labels = lbl.fit_transform(qualitativo).reshape(8,1)\n",
        "print(labels)\n",
        "print()\n",
        "print(\"Vermelho = 2\\nVerde = 1\\nAzul = 0\")\n",
        "print()\n",
        "print(enc.fit_transform(labels).toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2]\n",
            " [2]\n",
            " [1]\n",
            " [0]\n",
            " [2]\n",
            " [0]\n",
            " [0]\n",
            " [1]]\n",
            "\n",
            "Vermelho = 2\n",
            "Verde = 1\n",
            "Azul = 0\n",
            "\n",
            "[[0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urH-2yTo_0VH",
        "outputId": "651927c6-7919-4b2a-caa4-3bc7370b82f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "help(PolynomialFeatures)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class PolynomialFeatures in module sklearn.preprocessing._data:\n",
            "\n",
            "class PolynomialFeatures(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
            " |  Generate polynomial and interaction features.\n",
            " |  \n",
            " |  Generate a new feature matrix consisting of all polynomial combinations\n",
            " |  of the features with degree less than or equal to the specified degree.\n",
            " |  For example, if an input sample is two dimensional and of the form\n",
            " |  [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].\n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  degree : integer\n",
            " |      The degree of the polynomial features. Default = 2.\n",
            " |  \n",
            " |  interaction_only : boolean, default = False\n",
            " |      If true, only interaction features are produced: features that are\n",
            " |      products of at most ``degree`` *distinct* input features (so not\n",
            " |      ``x[1] ** 2``, ``x[0] * x[2] ** 3``, etc.).\n",
            " |  \n",
            " |  include_bias : boolean\n",
            " |      If True (default), then include a bias column, the feature in which\n",
            " |      all polynomial powers are zero (i.e. a column of ones - acts as an\n",
            " |      intercept term in a linear model).\n",
            " |  \n",
            " |  order : str in {'C', 'F'}, default 'C'\n",
            " |      Order of output array in the dense case. 'F' order is faster to\n",
            " |      compute, but may slow down subsequent estimators.\n",
            " |  \n",
            " |      .. versionadded:: 0.21\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> import numpy as np\n",
            " |  >>> from sklearn.preprocessing import PolynomialFeatures\n",
            " |  >>> X = np.arange(6).reshape(3, 2)\n",
            " |  >>> X\n",
            " |  array([[0, 1],\n",
            " |         [2, 3],\n",
            " |         [4, 5]])\n",
            " |  >>> poly = PolynomialFeatures(2)\n",
            " |  >>> poly.fit_transform(X)\n",
            " |  array([[ 1.,  0.,  1.,  0.,  0.,  1.],\n",
            " |         [ 1.,  2.,  3.,  4.,  6.,  9.],\n",
            " |         [ 1.,  4.,  5., 16., 20., 25.]])\n",
            " |  >>> poly = PolynomialFeatures(interaction_only=True)\n",
            " |  >>> poly.fit_transform(X)\n",
            " |  array([[ 1.,  0.,  1.,  0.],\n",
            " |         [ 1.,  2.,  3.,  6.],\n",
            " |         [ 1.,  4.,  5., 20.]])\n",
            " |  \n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  powers_ : array, shape (n_output_features, n_input_features)\n",
            " |      powers_[i, j] is the exponent of the jth input in the ith output.\n",
            " |  \n",
            " |  n_input_features_ : int\n",
            " |      The total number of input features.\n",
            " |  \n",
            " |  n_output_features_ : int\n",
            " |      The total number of polynomial output features. The number of output\n",
            " |      features is computed by iterating over all suitably sized combinations\n",
            " |      of input features.\n",
            " |  \n",
            " |  Notes\n",
            " |  -----\n",
            " |  Be aware that the number of features in the output array scales\n",
            " |  polynomially in the number of features of the input array, and\n",
            " |  exponentially in the degree. High degrees can cause overfitting.\n",
            " |  \n",
            " |  See :ref:`examples/linear_model/plot_polynomial_interpolation.py\n",
            " |  <sphx_glr_auto_examples_linear_model_plot_polynomial_interpolation.py>`\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      PolynomialFeatures\n",
            " |      sklearn.base.TransformerMixin\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, degree=2, interaction_only=False, include_bias=True, order='C')\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  fit(self, X, y=None)\n",
            " |      Compute number of output features.\n",
            " |      \n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like, shape (n_samples, n_features)\n",
            " |          The data.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : instance\n",
            " |  \n",
            " |  get_feature_names(self, input_features=None)\n",
            " |      Return feature names for output features\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      input_features : list of string, length n_features, optional\n",
            " |          String names for input features if available. By default,\n",
            " |          \"x0\", \"x1\", ... \"xn_features\" is used.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      output_feature_names : list of string, length n_output_features\n",
            " |  \n",
            " |  transform(self, X)\n",
            " |      Transform data to polynomial features\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like or CSR/CSC sparse matrix, shape [n_samples, n_features]\n",
            " |          The data to transform, row by row.\n",
            " |      \n",
            " |          Prefer CSR over CSC for sparse input (for speed), but CSC is\n",
            " |          required if the degree is 4 or higher. If the degree is less than\n",
            " |          4 and the input format is CSC, it will be converted to CSR, have\n",
            " |          its polynomial features generated, then converted back to CSC.\n",
            " |      \n",
            " |          If the degree is 2 or 3, the method described in \"Leveraging\n",
            " |          Sparsity to Speed Up Polynomial Feature Expansions of CSR Matrices\n",
            " |          Using K-Simplex Numbers\" by Andrew Nystrom and John Hughes is\n",
            " |          used, which is much faster than the method used on CSC input. For\n",
            " |          this reason, a CSC input will be converted to CSR, and the output\n",
            " |          will be converted back to CSC prior to being returned, hence the\n",
            " |          preference of CSR.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      XP : np.ndarray or CSR/CSC sparse matrix, shape [n_samples, NP]\n",
            " |          The matrix of features, where NP is the number of polynomial\n",
            " |          features generated from the combination of inputs.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  powers_\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.TransformerMixin:\n",
            " |  \n",
            " |  fit_transform(self, X, y=None, **fit_params)\n",
            " |      Fit to data, then transform it.\n",
            " |      \n",
            " |      Fits transformer to X and y with optional parameters fit_params\n",
            " |      and returns a transformed version of X.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : numpy array of shape [n_samples, n_features]\n",
            " |          Training set.\n",
            " |      \n",
            " |      y : numpy array of shape [n_samples]\n",
            " |          Target values.\n",
            " |      \n",
            " |      **fit_params : dict\n",
            " |          Additional fit parameters.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      X_new : numpy array of shape [n_samples, n_features_new]\n",
            " |          Transformed array.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self, N_CHAR_MAX=700)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  get_params(self, deep=True)\n",
            " |      Get parameters for this estimator.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      deep : bool, default=True\n",
            " |          If True, will return the parameters for this estimator and\n",
            " |          contained subobjects that are estimators.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      params : mapping of string to any\n",
            " |          Parameter names mapped to their values.\n",
            " |  \n",
            " |  set_params(self, **params)\n",
            " |      Set the parameters of this estimator.\n",
            " |      \n",
            " |      The method works on simple estimators as well as on nested objects\n",
            " |      (such as pipelines). The latter have parameters of the form\n",
            " |      ``<component>__<parameter>`` so that it's possible to update each\n",
            " |      component of a nested object.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **params : dict\n",
            " |          Estimator parameters.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          Estimator instance.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjrI6QfQ91rQ",
        "outputId": "42f04d1d-6647-424d-b5e3-33381f3558d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"\n",
        "O exemplo python a seguir usa o conjunto de dados de Boston para verificar \n",
        "a eficácia da técnica. Se for bem-sucedida, a expansão polinômial pegará \n",
        "relações não lineares em dados que requerem uma curva, não uma linha, \n",
        "para prever corretamente e superar qualquer dificuldade de previsão em \n",
        "detrimento de um número crescente de preditores.\n",
        "\"\"\"\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.model_selection  import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "Polinomial = PolynomialFeatures(degree=2)\n",
        "Polinomial_X = Polinomial.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(Polinomial_X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "regressao = Ridge(alpha=0.1, normalize=True)\n",
        "regressao.fit(X_train,y_train)\n",
        "print(f'R2: {r2_score(y_test,regressao.predict(X_test)):.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "R2: 0.820\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_FXQ1JxFCFH",
        "outputId": "14dd9ced-b50b-4359-f22d-b117f128549f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"\n",
        "Uma solução para um problema envolvendo uma resposta binária\n",
        "(o modelo tem que escolher entre duas classes possíveis) \n",
        "seria codificar um vetor de resposta como uma sequência de uns e zeros \n",
        "(ou valores positivos e negativos). \n",
        "O seguinte código Python prova a viabilidade e os limites do uso\n",
        "de uma resposta binária:\n",
        "\"\"\"\n",
        "import numpy as np \n",
        "a = np.array([0, 0, 0, 0, 1, 1, 1, 1]) \n",
        "b = np.array([1, 2, 3, 4, 5, 6, 7, 8]).reshape(8,1) \n",
        "from sklearn.linear_model import LinearRegression \n",
        "regressao = LinearRegression() \n",
        "regressao.fit(b,a) \n",
        "print (regressao.predict(b)>0.5) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[False False False False  True  True  True  True]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vw5mF47BLnj6",
        "outputId": "0b07d7b7-e19d-4a80-d520-6f539a98bacb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\"\"\"\n",
        "A regressão logística é a mesma que uma regressão linear, \n",
        "exceto que os dados y contêm números inteiros indicando a classe \n",
        "em relação à observação. Assim, usando o conjunto de dados de Boston \n",
        "a partir do módulo de conjuntos de dados Scikit-learn, você pode tentar \n",
        "adivinhar o que torna as casas em uma área excessivamente cara\n",
        "(valores medianos >= 40):\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "binary_y = np.array(y >= 40).astype(int)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,binary_y, test_size=0.33, random_state=5)\n",
        "\n",
        "logistic = LogisticRegression()\n",
        "logistic.fit(X_train,y_train)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(f'Treinamento: {accuracy_score(y_train, logistic.predict(X_train)):.3f}')\n",
        "print(f'Teste:{accuracy_score(y_test, logistic.predict(X_test)):.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Treinamento: 0.979\n",
            "Teste:0.958\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNOyjxFGMK2B",
        "outputId": "d9255aab-7926-42d8-8630-ec7c1b7bf02a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "\"\"\"\n",
        "O exemplo divide os dados em conjuntos de treinamento e teste, permitindo \n",
        "verificar a eficácia do modelo de regressão logística em dados que o modelo \n",
        "não usou para aprender. Os coeficientes resultantes dizem a probabilidade de uma\n",
        " determinada classe estar na classe-alvo\n",
        " (que é qualquer classe codificada usando um valor de 1). \n",
        " Se um coeficiente aumentar a probabilidade, terá um coeficiente positivo; \n",
        " caso contrário, o coeficiente é negativo.\n",
        "\"\"\"\n",
        "for var,coef in zip(boston.feature_names,\n",
        " logistic.coef_[0]):\n",
        " print (f\"{var} : {coef:.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CRIM : 0.086\n",
            "ZN : 0.230\n",
            "INDUS : 0.580\n",
            "CHAS : -0.029\n",
            "NOX : -0.304\n",
            "RM : 1.769\n",
            "AGE : -0.127\n",
            "DIS : -0.539\n",
            "RAD : 0.919\n",
            "TAX : -0.165\n",
            "PTRATIO : -0.782\n",
            "B : 0.077\n",
            "LSTAT : -1.628\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f7lC1WEM2u-",
        "outputId": "a8ded24e-4d1a-402a-c337-ef29d4bb0b82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "\"\"\"\n",
        "Lendo os resultados na sua tela, você pode ver que em Boston, a criminalidade (CRIM)\n",
        " tem algum efeito sobre os preços. No entanto, o nível de pobreza (LSTAT), \n",
        " a distância do trabalho (DIS) e a poluição (NOX) têm efeitos muito maiores. \n",
        " Além disso, ao contrário da regressão linear, a regressão logística não produz \n",
        " simplesmente a classe resultante (neste caso um 1 ou um 0), mas também estima a \n",
        " probabilidade de a observação fazer parte de uma das duas classes:\n",
        "\"\"\"\n",
        "print('\\nclasses:',logistic.classes_)\n",
        "print('\\nProbs:\\n',logistic.predict_proba(X_test)[:3,:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "classes: [0 1]\n",
            "\n",
            "Probs:\n",
            " [[0.33234217 0.66765783]\n",
            " [0.97060356 0.02939644]\n",
            " [0.99594746 0.00405254]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqjZ28cmN7wm",
        "outputId": "82decca1-7356-40b9-ff69-deacefc9a5b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "\"\"\"\n",
        "A menos que você use validação cruzada, medidas de erro como R2 podem ser enganosas\n",
        "porque o número de features pode facilmente inflar, mesmo que a feature não contenha \n",
        "informações relevantes. O exemplo a seguir mostra o que acontece com o R2 quando você \n",
        "adiciona apenas features aleatórias:\n",
        "\"\"\"\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "check = [2**i for i in range(8)]\n",
        "\n",
        "for i in range(2**7+1):\n",
        "  X_train = np.column_stack((X_train,np.random.random(X_train.shape[0])))\n",
        "  X_test = np.column_stack((X_test,np.random.random(X_test.shape[0])))\n",
        "  regressao.fit(X_train, y_train)\n",
        "  \n",
        "  if i in check:\n",
        "    print (f\"Random features: {i} -> R2: {r2_score(y_train,regressao.predict(X_train)):.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random features: 1 -> R2: 0.744\n",
            "Random features: 2 -> R2: 0.744\n",
            "Random features: 4 -> R2: 0.748\n",
            "Random features: 8 -> R2: 0.750\n",
            "Random features: 16 -> R2: 0.757\n",
            "Random features: 32 -> R2: 0.768\n",
            "Random features: 64 -> R2: 0.789\n",
            "Random features: 128 -> R2: 0.843\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nKRRCbMPBYv",
        "outputId": "9dcec4e6-07b3-4d29-ad57-805b427aa530",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"\n",
        "O que parece ser uma capacidade preditiva aumentada é apenas uma ilusão. Você pode revelar o que aconteceu verificando o conjunto de testes e descobrindo que o desempenho do modelo diminuiu:\n",
        "a)Observe que o resultado R2 pode mudar de execução\n",
        "b)devido à natureza aleatória do experimento R2 0.474\n",
        "\"\"\"\n",
        "\n",
        "regressao.fit(X_train, y_train)\n",
        "print(f'R2 {r2_score(y_test,regression.predict(X_test)):.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "R2 0.559\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KJgeB7hRtUV",
        "outputId": "814f107d-a6ea-4e9a-b357-a2ae2b78a254",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"\n",
        "O exemplo a seguir modifica o exemplo de expansões polinomiais \n",
        "usando a regularização L2 e reduz a influência de coeficientes redundantes \n",
        "criados pelo procedimento de expansão:\n",
        "\"\"\"\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "pf = PolynomialFeatures(degree=2)\n",
        "poly_X = pf.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(poly_X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "reg_regression = Ridge(alpha=0.1, normalize=True)\n",
        "reg_regression.fit(X_train,y_train)\n",
        "print (f'R2: {r2_score(y_test,reg_regression.predict(X_test)):.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "R2: 0.820\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqnH_93F3atM",
        "outputId": "e2e40386-804a-43c6-f6e5-5075ce1fb895",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "\"\"\"\n",
        "Para demonstrar a eficácia do aprendizado fora do núcleo, \n",
        "o exemplo a seguir estabelece um breve experimento em Python usando \n",
        "regressão e squared_loss como função de custo. Ele se baseia no \n",
        "dataset Boston depois de embaralhá-lo e separá-lo em conjuntos de \n",
        "treinamento e testes. O exemplo demonstra como os coeficientes beta \n",
        "mudam à medida que o algoritmo vê mais exemplos.\n",
        "\"\"\"\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "SGD = SGDRegressor(penalty=None, learning_rate='invscaling', eta0=0.01, power_t=0.25, max_iter=5, tol=None)\n",
        "power = 17\n",
        "check = [2**i for i in range(power+1)]\n",
        "\n",
        "for i in range(400):\n",
        " for j in range(X_train.shape[0]):\n",
        "  SGD.partial_fit(X_train[j,:].reshape(1,13), y_train[j].reshape(1,))\n",
        "  count = (j+1) + X_train.shape[0] * i\n",
        "  if count in check:\n",
        "    R2 = r2_score(y_test,SGD.predict(X_test))\n",
        "    print (f'Exemplo {count} R2 {R2:.3f} coef:'+' '.join(map( lambda x:'%0.3f' %x, SGD.coef_)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exemplo 1 R2 -6.255 coef:0.112 -0.071 0.148 -0.040 0.075 -0.021 0.146 -0.113 0.243 0.224 0.118 0.037 0.110\n",
            "Exemplo 2 R2 -6.168 coef:0.065 -0.139 0.087 -0.078 0.055 -0.114 0.254 -0.054 0.154 0.140 0.282 0.068 0.152\n",
            "Exemplo 4 R2 -6.060 coef:-0.074 -0.195 0.319 -0.171 0.064 -0.206 0.527 0.048 -0.041 0.266 0.075 0.219 0.353\n",
            "Exemplo 8 R2 -5.775 coef:-0.249 -0.504 0.605 -0.343 0.098 0.005 0.807 -0.304 -0.095 0.332 -0.067 0.399 0.024\n",
            "Exemplo 16 R2 -5.144 coef:-0.441 -0.430 0.298 -0.571 -0.002 0.004 0.519 -0.423 -0.279 0.292 -0.544 0.665 -0.065\n",
            "Exemplo 32 R2 -4.494 coef:-0.562 -0.308 0.441 1.224 0.051 0.315 0.387 -0.567 0.055 0.629 -0.367 0.726 -0.513\n",
            "Exemplo 64 R2 -2.947 coef:-0.986 0.419 0.107 1.648 -0.409 1.686 -0.427 -0.201 -0.029 0.448 -1.245 1.166 -1.913\n",
            "Exemplo 128 R2 -1.791 coef:-0.546 0.863 0.119 1.137 -0.584 1.823 -0.288 -0.179 -0.281 0.096 -1.982 1.165 -2.029\n",
            "Exemplo 256 R2 -0.608 coef:-0.804 0.619 -0.176 1.368 -0.770 3.135 -0.304 -0.514 -0.318 -0.201 -2.325 1.238 -2.758\n",
            "Exemplo 512 R2 0.289 coef:-0.665 0.455 0.167 1.302 -0.570 3.073 -0.065 -1.175 0.163 0.223 -2.238 1.074 -2.937\n",
            "Exemplo 1024 R2 0.626 coef:-0.775 0.302 0.178 1.177 -0.757 3.379 -0.176 -1.477 0.308 0.216 -2.190 1.125 -3.283\n",
            "Exemplo 2048 R2 0.698 coef:-0.803 0.316 0.161 1.012 -1.068 3.231 -0.303 -1.886 0.537 0.116 -2.028 1.119 -3.565\n",
            "Exemplo 4096 R2 0.709 coef:-0.869 0.424 0.169 0.973 -1.362 2.988 -0.350 -2.365 0.809 -0.074 -1.931 1.101 -3.739\n",
            "Exemplo 8192 R2 0.715 coef:-0.964 0.638 0.140 0.902 -1.648 2.953 -0.420 -2.688 1.120 -0.404 -1.976 1.084 -3.881\n",
            "Exemplo 16384 R2 0.722 coef:-1.015 0.788 0.231 0.819 -1.800 2.713 -0.395 -2.891 1.480 -0.778 -1.991 1.069 -3.893\n",
            "Exemplo 32768 R2 0.721 coef:-1.081 0.844 0.271 0.872 -1.894 2.779 -0.386 -2.977 1.779 -1.162 -2.003 1.096 -3.949\n",
            "Exemplo 65536 R2 0.726 coef:-1.093 0.890 0.391 0.802 -1.882 2.697 -0.368 -2.981 1.982 -1.315 -2.020 1.077 -3.904\n",
            "Exemplo 131072 R2 0.724 coef:-1.103 0.892 0.371 0.848 -1.899 2.758 -0.373 -3.003 2.027 -1.398 -2.009 1.090 -3.950\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qi2TfvO5rIX"
      },
      "source": [
        "Lembre-se: Não importa a quantidade de dados, você sempre pode se encaixar em um modelo de regressão linear simples, mas eficaz, usando features de aprendizagem on-line SGD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZXrHjU650g2"
      },
      "source": [
        "# ATIVIDADE PRÁTICA: Linear/Polynomial regression\n",
        "Com o dataset carregado, crie:\n",
        "1. Modelo preditivo utilizando Regressão Linear(utilize dataviz+scores como R2)\n",
        "2. Modelo preditivo utilizando Regressão Polinomial(utilize dataviz+scores como R2)\n",
        "<br><br>Ao término, através do score R2, comprove cientificamente qual melhor algoritmo para o dataset compartilhado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5EAzgmXnPU1",
        "outputId": "25210c7a-6cd1-4e7f-9ebf-dab3aad05a07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "# LIBS\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# DATASET\n",
        "np.random.seed(0)\n",
        "x = 2 - 3 * np.random.normal(0, 1, 20)\n",
        "y = x - 2 * (x ** 2) + 0.5 * (x ** 3) + np.random.normal(-3, 3, 20)\n",
        "\n",
        "# ADICIONAMOS NOVA COLUNA AO INVÉS DE RESHAPE COM NP.NEWAXIS\n",
        "x = x[:, np.newaxis]\n",
        "y = y[:, np.newaxis]\n",
        "\n",
        "# DATAVIZ\n",
        "plt.scatter(x,y, s=10)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJaM7iZU-fCF"
      },
      "source": [
        "# 1 LINEAR REGRESSION\n",
        "\"\"\"\n",
        "Dicas modelo preditivo: \n",
        "a)importe seu algoritmo Linear Regression, \n",
        "b)instancie ele em uma variável, \n",
        "c)utilize .fit() para treinar\n",
        "d) instancie uma variável para .predict()\n",
        "e) instancie RMSE e R2\n",
        "\n",
        "Dicas DataViz:\n",
        "a) Utilize .scatter()\n",
        "b) Utilize .plot()\n",
        "c) utilize .show()\n",
        "\n",
        "Visite as documentações\n",
        "\"\"\"\n",
        "# importações essenciais:\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwS02c6J-g8r"
      },
      "source": [
        "# 2 POLYNOMIAL REGRESSION\n",
        "\"\"\"\n",
        "a) instancie a PolynomialFeatures em uma variável \n",
        "b) nesta variável, use .fit_transform() para trata-la \n",
        "c) instancie seu algoritmo de Regressão Linear em uma variável\n",
        "d) use o .fit() para treina-lo \n",
        "e) instancie em uma variável o .predict()\n",
        "f) instancie RMSE e R2\n",
        "g) utilize .scatter(), .plot() e .show() para dataviz\n",
        "\n",
        "Visite as documentações\n",
        "\"\"\"\n",
        "\n",
        "# importações essenciais:\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import PolynomialFeatures"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}